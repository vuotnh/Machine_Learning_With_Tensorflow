{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a DeepSpeech LSTM Model using the LibriSpeech Data \n",
    "At the end of Chapter 16 and into Chapter 17 in the book it is suggested try and build an automatic speech recognition system using the LibriVox corpus and long short term memory (LSTM) models just learned in the Recurrent Neural Network (RNN) chapter. This particular excercise turned out to be quite difficult mostly from the perspective again of simply gathering and formatting the data, combined with the work to understand the LSTM was doing. As it turns out, in doing this assignment I taught myself about MFCCs (mel frequency cepstral coefficient) which are simply what is going on in the  Bregman Toolkit example earlier in the book. It's a process to convert audio into *num_cepstrals* coefficients using an FFT, and to use those coeffiicents as amplitudes and convert from the frequency into the time domain. LSTMs need time series data and a number of audio files converted using MFCCs into frequency amplitudes corresponding to utterances that you have transcript data for and you are in business!\n",
    "\n",
    "The other major lesson was finding [RNN-Tutorial](https://github.com/mrubash1/RNN-Tutoria) an existing GitHub repository that implements a simplified version of the [deepspeech model](https://github.com/mozilla/DeepSpeech) from Mozilla which is a TensorFlow implementation of the Baidu model from the [seminal paper](https://arxiv.org/abs/1412.5567) in 2014.\n",
    "\n",
    "I had to figure out along the way how to tweak hyperparameters including epochs, batch size, and training data. But overall this is a great architecture and example of how to use validation/dev sets during training for looking at validation loss compared to train loss and then overall to measure test accuracy.\n",
    "\n",
    "### Data Preprocessing Steps:\n",
    "   1. Grab all text files which start out as the full speech from all subsequent \\*.flac files\n",
    "   2. Each line in the text file contains:\n",
    "       ```\n",
    "       filename(without .txt at end) the speech present in the file, e.g., words separated by spaces\n",
    "       filename N ... words ....\n",
    "       ```\n",
    "   3. Then convert all \\*.flac files to \\*.wav files, using `flac2wav`\n",
    "   4. Remove all the flac files and remove the \\*.trans.txt files\n",
    "   5. Run this code in the notebook below to generate the associated \\*.txt file to go along with each \\*.wav file.\n",
    "   6. Move all the \\*.wav and \\*.txt files into a single folder, e.g., `LibriSpeech/train-clean-all` \n",
    "   7. Repeat for test and dev\n",
    "   \n",
    "Once complete, you have a dataset to run through [RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial.git)\n",
    "\n",
    "### References\n",
    "   1. [PyDub](https://github.com/jiaaro/pydub) - PyDub library\n",
    "   2. [A short reminder of how CTC works](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7)\n",
    "   3. [OpenSLR - LibriSpeech corpus](http://www.openslr.org/12)\n",
    "   4. [Hamsa's Deep Speech notebook](https://github.com/cosmoshsv/Deep-Speech/blob/master/DeepSpeech_RNN_Training.ipynb)\n",
    "   5. [LSTM's by example using TensorFlow](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537) \n",
    "   6. [How to read an audio file using TensorFlow APIs](https://github.com/tensorflow/tensorflow/issues/28237)\n",
    "   7. [Audio spectrograms in TensorFlow](https://mauri870.github.io/blog/posts/audio-spectrograms-in-tensorflow/)\n",
    "   8. [Reading audio files using TensorFlow](https://github.com/tensorflow/tensorflow/issues/32382)\n",
    "   9. [TensorFlow's decode_wav API](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav)\n",
    "   10. [Speech Recognition](https://towardsdatascience.com/speech-recognition-analysis-f03ff9ce78e9)\n",
    "   11. [Using TensorFlow's audio ops](https://stackoverflow.com/questions/48660391/using-tensorflow-contrib-framework-python-ops-audio-ops-audio-spectrogram-to-gen)\n",
    "   12. [LSTM by Example - Towards Data Science](https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537)\n",
    "   13. [Training your Own Model -  DeepSpeech](https://deepspeech.readthedocs.io/en/v0.7.3/TRAINING.html)\n",
    "   14. [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "   15. [Implementing  LSTMs](https://apaszke.github.io/lstm-explained.html)\n",
    "   16. [Mel Frequency Cepstral Coefficient](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)\n",
    "   17. [TensorFlow - Extract Every Other Element](https://stackoverflow.com/questions/46721407/tensorflow-extract-every-other-element)\n",
    "   18. [Plotting MFCCs in TensorFlow](https://stackoverflow.com/questions/47056432/is-it-possible-to-get-exactly-the-same-results-from-tensorflow-mfcc-and-librosa)\n",
    "   19. [MFCCs in TensorFlow](https://kite.com/python/docs/tensorflow.contrib.slim.rev_block_lib.contrib_framework_ops.audio_ops.mfcc)\n",
    "   20. [How to train Baidu's Deep Speech Model with Kur](https://blog.deepgram.com/how-to-train-baidus-deepspeech-model-with-kur/)\n",
    "   21. [Silicon Valley Data Science SVDS - RNN Tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)\n",
    "   22. [Streaming RNNs with TensorFlow](https://hacks.mozilla.org/2018/09/speech-recognition-deepspeech/)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../libs/basic_units/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.audio import decode_wav\n",
    "from tensorflow.raw_ops import Mfcc, AudioSpectrogram\n",
    "from tqdm.notebook import tqdm\n",
    "from basic_units import cm, inch\n",
    "import glob\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import os\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_data_path = \"../data/LibriSpeech\"\n",
    "train_path = speech_data_path + \"/train-clean-100\"\n",
    "dev_path = speech_data_path + \"/dev-clean\"\n",
    "test_path = speech_data_path + \"/test-clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transcripts = [file for file in glob.glob(train_path + \"/*/*/*.txt\")]\n",
    "dev_transcripts = [file for file in glob.glob(dev_path + \"/*/*/*.txt\")]\n",
    "test_transcripts = [file for file in glob.glob(test_path + \"/*/*/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_wav = [file for file in glob.glob(train_path + \"/*/*/*.wav\")]\n",
    "dev_audio_wav = [file for file in glob.glob(dev_path + \"/*/*/*.wav\")]\n",
    "test_audio_wav = [file for file in glob.glob(test_path + \"/*/*/*.wav\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../libs/RNN-Tutorial/src\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcep=26\n",
    "numcontext=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163840, 1)\n",
      "16000\n",
      "[[[-1.2924805e+00 -1.6727712e+00  2.6565906e-01 ... -4.9464533e-01\n",
      "   -8.0459461e-02 -2.4568604e-01]\n",
      "  [-1.3827541e+00 -1.6454916e+00  2.7615392e-01 ... -4.9200761e-01\n",
      "   -8.6767256e-02 -2.2917633e-01]\n",
      "  [-1.5100304e+00 -1.5856098e+00  3.4048718e-01 ... -4.5024821e-01\n",
      "   -7.9039715e-02 -1.7609477e-01]\n",
      "  ...\n",
      "  [-3.8997927e+00 -9.6423161e-01 -5.6239200e-01 ... -2.0234044e-01\n",
      "   -1.3896421e-01  4.8706021e-02]\n",
      "  [-3.8434243e+00 -9.2779779e-01 -5.9620500e-01 ... -1.0271078e-01\n",
      "   -9.9897847e-02  1.9752394e-02]\n",
      "  [-3.8488970e+00 -9.0062475e-01 -6.3511568e-01 ...  2.8306339e-03\n",
      "   -5.6673948e-02 -5.8116550e-03]]]\n",
      "(1, 2545, 26)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename =  '../data/LibriSpeech/train-clean-100/3486/166424/3486-166424-0004.wav'\n",
    "raw_audio = tf.io.read_file(filename)\n",
    "audio, fs = decode_wav(raw_audio)\n",
    "print(np.shape(audio.numpy()))\n",
    "print(fs.numpy())\n",
    "\n",
    "# Get mfcc coefficients\n",
    "spectrogram = AudioSpectrogram(\n",
    "        input=audio, window_size=1024,stride=64)\n",
    "orig_inputs = Mfcc(spectrogram=spectrogram, sample_rate=fs, dct_coefficient_count=numcep)\n",
    "\n",
    "audio_mfcc = orig_inputs.numpy()\n",
    "print(audio_mfcc)\n",
    "print(np.shape(audio_mfcc))\n",
    "hist_audio = np.histogram(audio_mfcc, bins=range(9 + 1))\n",
    "plt.hist(hist_audio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15  1.15  2.15  3.15  4.15  5.15  6.15  7.15  8.15  9.15 10.15 11.15\n",
      " 12.15 13.15 14.15 15.15 16.15 17.15 18.15 19.15 20.15 21.15 22.15 23.15\n",
      " 24.15 25.15]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAw3klEQVR4nO3debgcVZ3/8feHEBL2LQECWTGgbDFiBnQUCSoIyoAoCogOMCIqwzLu2/yAoIwK48ZEhQghCIqoCEZFAYUIoiwJhkBYJCKaG0CSsARIIAvf3x/nNKnc9FL3pvvevjef1/PU091Vp6pOrd+uc6pOKSIwMzPrbIPezoCZmbUnBwgzM6vKAcLMzKpygDAzs6ocIMzMrCoHCDMzq8oBoh+QNEPSifn7sZKub/L0R0sKSRvWGP5KSbMlPSvptGbO25pP0vaSbs7b62u9nZ/OJH1e0kW9nY/eUDyW24EDRAmSHpH0hKRNC/1OlDSjF7NVVUT8ICIO6uHZfhq4KSI2j4jze3je66V1PJGcBCwCtoiIT6xjPl4n6XlJm1UZ9mdJp3R1mhHxPxHRoydJSYfnPzlLJC2SdKOkMT2Zh3bkAFHeAOD0dZ2Ikv623kcBc2sNlDSgB/NijY0C7otuPCXb+SoyIm4DOoAjO6XbE9gduGJdpt8TJI0Fvg98AtgSGAN8G1jV03lpOxHhrkEHPAJ8FngS2Cr3OxGYUUjzr8CdwDP5818Lw2YA5wC3AsuAsUAAJwMPAc8CXwReAfwRWAL8GNgoj7818EtgIfBU/j680/RPzN+PB/6Qv38aeK7QrQCm5WFbAhcDjwELgC8BA/KwAcD/kv5lPgz8Z87vhlXWzY2kA+mFPI9dgWnAd4FrgeeBtwI7AlflZfgbcFphGhvncZ4C7gM+BXQUhgcwtvB7GvClwu9DgdnA03n9jeu07T4JzMnb5kpgcGH44XncJcBfgYOB9wCzOi3nx4Gf19g/tgEuAR7Ny3BNYdiHgHmkfWc6sGPuP7rzOq22HfN2eCqvs0PysHM6rfPJgIBvAE/kZbkH2LNKXqfl/WB5HvetwCDgmzn/j+bvg3L6iaQA8BngceCyKtP8PHBjp37nAlfn798C5ud8zQL2K6Q7C/gpcHkefmLud3khzWGkPyBP53W0W5l9AxhCOlaezuv/FmCDKvk/Ephd5/ivt36OJx9v1fKU8/Nt4Fek4/x24BWFtAcCD5D2zcnA7wv7wNj8+xnSsXhlj5/7enqGfbEjnWTeCvyssPO9HCBIJ4ingA8AGwLH5N/b5uEzgH8Ae+ThA/NO9HNgi9z/ReB3wM6kk/d9wHF5/G2BdwObAJsDP2HNk9AMqgSITsswIu/clZPM1cCFwKbAdsAdwIfzsI/knXZEXrabqBEgOs+/cFA8A7yBdJW6CenEcAawUV7Gh4G35fRfIR282+R53kvJAAG8hnRS3JcU2I7L22tQYdvdQQpQ2wD3Ax/Jw/bJ+Tww53Mn4FWkE8KTrHki+jPw7hrL/ytS4Nk6b9v9c/83kw7svfM0/w+4OQ8b3XmdVtmOK0gBZgDw0bz9VGOdvy2v461IwWI3YFiN/L68/vLvs4Hb8n4wlBRkv5iHTQRWAl/Ny7BxjX1rJTAi/96AFFTemX+/n7QPb0j6l/44OUiTgsEK4J15vI0pBAjSH47n8zYaSPrTM4/Vf57q7RtfBi7I4w0E9qusv07535kUbL8BHABs1ml4vfVzPI0DxGLSvrYh8APgR3nYEFLQODLn72N5PVb2gSuAL+T1Mhh4Y0+f+/pbUUernQGcKmlop/7vAB6KiMsiYmVEXEE6wf5bIc20iJibh6/I/c6NiCURMZd0Urw+Ih6OiGeAX5NOfkTE4oi4KiKWRsSzpH+Q+5fNtKSNgWuAb0XEryVtD7wd+K+IeD4iniAdHEfnUd4LfDMi5kfEk6QDrat+HhG3RsRLwF7A0Ig4OyKWR8TDwPc6ze+ciHgyIuYDXanHOAm4MCJuj4hVEXEpKdi+rpDm/Ih4NC/LL4Dxuf8HgakRcUNEvBQRCyLigYh4kXTCfz+ApD1IJ/Rfdp65pGHAIaSg81RErIiI3+fBx+bp35Wn+Tng9ZJGl1y2v0fE9yJiFXApMAzYvkbaFaQ/D68inQTvj4jHSs7nWODsiHgiIhYCk0h/dipeAs6MiBcjYlnnkfM2m1EY5y2kYPKrPPzyvA+vjIiv5WGvLEziTxFxTd4Gnad/FPCrvI1WkK6oNiZdsTeygrTORuXtckvkM2+n/D9MCoQ7ka7cF0maVqhXabR+Grk6Iu6IiJWkADE+9387MDcifpqX7Zuk4FnM/yjSVecLEfGHLsyzKRwguiAi7iWdJD7badCOwN879fs7aYermF9lkv8sfF9W5fdmAJI2kXShpL9LWgLcDGzVhbL9i4EHI+Kr+fco0j+WxyQ9Lelp0tXEdoXlKea387KVURx/FLBjZV55fp9n9cluXeY3CvhEp2mPyNOsKB50S8nrNaf7a43pXgq8T5JIJ4Mf55N8ZyOAJyPiqSrD1tgvIuI50r/JnaqkreblfEfE0vx1rcrgPPxGUhHFt4EnJE2RtEXJ+XTef//OmutvYUS80GAal7L6pPkB0r/kFQCSPinpfknP5O2zJenfc0W1Y6Nq3vIfjvmUW4fnka42rpf0sKTOx+3LIuK2iHhvRAwlXWm8ifTvfa08sPb6aaTW/rfGfp+DV3FdfJp0NXiHpLmS/qML82wKB4iuO5N02V/cQR8lnaiKRpLK9ivW+ufSBZ8g/ePaNyK2IO28kHaeuvJBsSvp33LFfNK/7CERsVXutoiIPfLwx0gnvoqR3chzcXnnA38rzGurSHc8vb3k/JaSiqkqdug07XM6TXuTfBXXyHxSvc/amU+Vr8tJJ4v3AZfVmcY2kraqMmyN/SLfBbctab94PveutVyNVPsnfH5EvJZUObwrqS6njM7778jcr+a8qvgZMFzSAcC7SAEDSfuRTnTvBbaOiK1IxXrFfbfe9DuvQ5H2lcqxVXPfiIhnI+ITEbEzqR7j45Le0mhBIuLOvDx7VssDa66f54vzl9SVbbjGfl9Ytko+Ho+ID0XEjsCHge/kCvUe4wDRRRExj1T8ULzf/1pgV0nvk7ShpKNIB+laRRLdtDnpiuJpSduQglRDkg7J+TyieOmeix6uB74maQtJG0h6haRKsdWPgdMkDZe0NWtfMXXVHcCzkj4jaWNJAyTtKelfCvP7nKStJQ0HTu00/mzSv/kBkg5mzeK17wEfkbRvvkNsU0nvkLR5iXxdDJwg6S15Hewk6VWF4d8n/StfUevyPq/LX5MO3q0lDZRUCeBX5OmPlzQI+B/g9oh4JBdVLADen5frP6gRrGr4J6nsHABJ/5LXwUDSSesFUtFQGVcA/y1pqKQhpKLUy7uQFyLieVJl8yWkorGZedDmpHL1hcCGks4g1buV9WPgHXkbDST9WXqRVA8AdfYNSYdKGptPvM+QKvbXWieS3ijpQ5K2y79fRQoot+Uk9dbP3cAeeRsPJtWflPWrPO678t1bp1EIcJLek48HSHWaUS3/reQA0T1nkyp3gVRHQLqT5hOkIoRPA4dGxKImze+bpHLXRaSd9jclxzuKVKl2v6TncndBHvbvpArj+0g7309J5bWQTrrXkXb+u0j/protl6EfSip7/VtejotIRQ2QynT/noddz9r/1k8n1ec8TSoPvqYw7ZmkK7rJeTnmkSoOy+TrDuAEUv3LM6Q7Ror/FC8j/YtsdLL8AKm8+AFShfl/5en/Fvh/pLu3HiMFgKML432I9C9/MelGhT9S3reAIyU9Jel80kn3e6R18Pc8zfNKTutLwEzSnV73kLb5l7qQl4pLSevv+4V+15H217/kfL1A/SKlNUTEg6S6oP8j7Tf/BvxbRCzPSWruG8AuwG9Jd2v9CfhORNxUZTZPkwLCPZKey/m9mnQnFtRZPxHxF9L54LekOxJL1xPk88N7SDdpLM75vbWQ5F+A23OepgOn5/qSHlO5I8KsbUiaSLqLZXiDpK3Ox8akE/7eEfFQb+bFrDf4CsKsto8Cdzo42Pqqx59aNOsLJD1Cqkh9Z+/mxKz3uIjJzMyqchGTmZlV1a+KmIYMGRKjR4/u7WyYmfUZs2bNWpQfEFxLvwoQo0ePZubMmY0TmpkZAJJqtlzgIiYzM6vKAcLMzKpqWYCQNELSTZLuyw1NnV4ljSSdL2mepDmS9i4MO07SQ7k7rlX5NDOz6lpZB7ES+ERE3JXbxZkl6YaIuK+Q5hDS4+W7kNrz/y6wb6G9oQmk9kdmSZpeo8VMM7MesWLFCjo6OnjhhUaN27afwYMHM3z4cAYOHFh6nJYFiNyI2WP5+7OS7ie1gFoMEIcD38/N3N4maSul9vUnAjfk9vuRdAPpTV9den2hmVkzdXR0sPnmmzN69GhSG4B9Q0SwePFiOjo6GDOm/Ku2e6QOQukFKa8hvW6vaCfWbLirI/er1b/atE+SNFPSzIULFzYtz2Zmnb3wwgtsu+22fSo4AEhi22237fKVT8sDhNJbma4ivb1sSbOnHxFTImJCREwYOrTqrbxmZk3T14JDRXfy3dIAkdtvvwr4QURUazJ6AWu+KGZ47lerv5mZ9ZBW3sUk0gtZ7o+Ir9dINh3493w30+uAZ3LdxXXAQfkFLFsDB+V+ZmbtQ2puV8I555zDHnvswbhx4xg/fjy33347kydPZuzYsUhi0aJmvYamtXcxvYH0IpV7JM3O/T5Pfp1kRFxAehPb20kveVlKenkLEfGkpC8Cd+bxzq5UWPeGyqWZGzY0s970pz/9iV/+8pfcddddDBo0iEWLFrF8+XI22mgjDj30UCZOnNjU+bXyLqY/0OCdyfnupf+sMWwqMLUFWTMz65Mee+wxhgwZwqBBgwAYMmQIADvuuGNL5ucnqc3M+oiDDjqI+fPns+uuu3LyySfz+9//vqXzc4AwM+sjNttsM2bNmsWUKVMYOnQoRx11FNOmTWvZ/PpVa65mZv3dgAEDmDhxIhMnTmSvvfbi0ksv5fjjj2/JvHwFYWbWRzz44IM89NDqV6TPnj2bUaNGtWx+DhBmZt0V0dyugeeee47jjjuO3XffnXHjxnHfffdx1llncf755zN8+HA6OjoYN24cJ554YlMWr1+9k3rChAnRihcG+TZXMwO4//772W233Xo7G91WLf+SZkXEhGrpfQVhZmZVOUCYmVlVDhBmZlaVA4SZmVXlAGFmZlU5QJiZWVV+ktrMrJs0qbkvD4ozG99Kf8455/DDH/6QAQMGsMEGG3DhhRdy/vnnM3PmTAYOHMg+++zDhRde2KV3T9fiKwgzsz6i2Nz3nDlz+O1vf8uIESM49thjeeCBB7jnnntYtmwZF110UVPm5ysIM7M+okxz3/vssw8dHR1NmZ+vIMzM+ohGzX2vWLGCyy67jIMPPrgp83OAMDPrIxo1933yySfzpje9if32268p82tZEZOkqcChwBMRsWeV4Z8Cji3kYzdgaH7d6CPAs8AqYGWtdkLMzNY3tZr7njRpEgsXLuTCCy9s2rxaeQUxDah5nRMR50XE+IgYD3wO+H2n904fkIc7OJiZUbu574suuojrrruOK664gg02aN5pvZXvpL5Z0uiSyY8BrmhVXszMWqHMbanN9Nxzz3Hqqafy9NNPs+GGGzJ27FimTJnCDjvswKhRo3j9618PwLve9S7OOOOMdZ5fr9/FJGkT0pXGKYXeAVwvKYALI2JKr2TOzKyNvPa1r+WPf/zjWv1XrlzZkvn1eoAA/g24tVPx0hsjYoGk7YAbJD0QETdXG1nSScBJACNHjmx9bs3M1hPtcBfT0XQqXoqIBfnzCeBqYJ9aI0fElIiYEBEThg4d2tSMSakzM1sf9WqAkLQlsD/w80K/TSVtXvkOHATc2zs5NDNbf7XyNtcrgInAEEkdwJnAQICIuCAnOwK4PiKeL4y6PXB1fs3nhsAPI+I3rcqnmZlV18q7mI4pkWYa6XbYYr+HgVe3JldmZlZWO9RBmJlZG2qHu5jMzPqkGTOaexfLxInda+57ypQpzJw5k4hg1113Zdq0aWy22WbrnB9fQZiZ9RG1mvv+xje+wd13382cOXMYOXIkkydPbsr8fAXRXRJEzz5FaWbrt1rNfVdEBMuWLUNNuj/fVxAt0KyNY2ZWVK+57xNOOIEddtiBBx54gFNPPbUp83OAMDPrI+o1933JJZfw6KOPsttuu3HllVc2ZX4OEOtAk9T0d9KamdVTae570qRJTJ48mauuumqNYUcfffQa/daFA4SZWR9RrbnvkSNHMm/ePCDVQUyfPp1XvepVTZmfK6nNzLqpzG2pzVStue8LLriAI444giVLlhARvPrVr+a73/1uU+bnAGFm1kfUau771ltvbcn8XMRkZmZVOUCYmVlVLmJqgsrj9gcc4AfnzKz/8BWEmZlV1TBASBpUpp+ZmfUvZa4g/lSyn5mZ9SM1A4SkHSS9FthY0msk7Z27icAmPZVBM7N2VXlvfbO6Ms455xz22GMPxo0bx/jx47n99ttfHnbaaac1pZnvinqV1G8DjgeGA18v9H8W+HzTcmBmZqUUm/seNGgQixYtYvny5QDMnDmTp556qqnzq3kFERGXRsQBwPERcUChOywiftZowpKmSnpC0r01hk+U9Iyk2bk7ozDsYEkPSpon6bPdWjIzs36mWnPfO+64I6tWreJTn/oU5557blPnV6YO4neSTpP0dUnnV7oS400DDm6Q5paIGJ+7swEkDQC+DRwC7A4cI2n3EvMzM+vXajX3PXnyZA477DCGDRvW1PmVeQ7iWuA24B7gpbITjoibJY3uRp72AeZFxMMAkn4EHA7c141pdcvq1wj6uQYzax+V5r5vueUWbrrpJo466ihOO+00rr32WmbMmNH0+ZUJEIMj4uNNn3Pyekl3A48Cn4yIucBOwPxCmg5g31oTkHQScBLAyJEjW5RNM7P2UGnue+LEiey1114cc8wxbLvttowdOxaApUuXMnbs2JdbeF0XZYqYLpP0IUnDJG1T6dZ5znAXMCoiXg38H3BNdyYSEVMiYkJETBg6dGgTsmVm1p6qNff94Q9/mMcff5xHHnmERx55hE022aQpwQHKXUEsB84DvsDqMpcAdl6XGUfEksL3ayV9R9IQYAEwopB0eO5nZtZWevq19NWa+54yZUrL5lcmQHwCGBsRi5o5Y0k7AP+MiJC0D+lqZjHwNLCLpDGkwHA08L5mztvMrC+q1dx30XPPPde0+ZUJEPOApV2dsKQrgInAEEkdwJnAQICIuAA4EviopJXAMuDoiAhgpaRTgOuAAcDUXDdhZmY9qEyAeB6YLekm4MVKz4g4rd5IEXFMg+GTgck1hl1LunvKzMx6SZkAcQ3drEA2M7O+q2GAiIhLeyIjZmbWXhoGCEl/o8oTYxGxTncxmZlZeytTxDSh8H0w8B6gGc9BmJlZG2v4oFxELC50CyLim8A7Wp81M7P2JqmpXRnVmvs+/vjjGTNmDOPHj2f8+PHMnj27KctXpohp78LPDUhXFH6XtZlZD6vX3Pd5553HkUce2dT5lTnRf63wfSXwCPDepuaijWiSuGn/3s6FmdnaqjX33UplipiK74I4MCI+FBEPtjRXZma2llrNfQN84QtfYNy4cXzsYx/jxRdfrDOV8hoGCElb5ndBzMzd1yRt2ZS5m5lZaZXmvqdMmcLQoUM56qijmDZtGl/+8pd54IEHuPPOO3nyySf56le/2pT5lWnNdSrpNaPvzd0S4JKmzN3MzLqk0tz3pEmTmDx5MldddRXDhg1DEoMGDeKEE07gjjvuaMq8ytRBvCIi3l34PUnS7KbM3czMSnvwwQfZYIMN2GWXXYDU3PeoUaN47LHHGDZsGBHBNddcw5577tmU+ZUJEMskvTEi/gAg6Q2kxvXMzNZr0cPtfddq7vu9730vCxcuJCIYP348F1xwQVPmVyZAfBS4tFDv8BRwfFPmbmZmpdVq7vvGG29syfzKtMU0G3i1pC3y7yX1xzAzs/6gzF1M/yNpq4hYEhFLJG0t6Us9kTkzM+s9Ze5iOiQinq78iIingLe3LEdmZm2sp+sdmqU7+S4TIAZIGlT5IWljYFCd9H2TlDozsxoGDx7M4sWL+1yQiAgWL17M4MGDuzRemUrqHwC/k1R59uEEoOE7IiRNBQ4FnoiIte65knQs8BlApOcsPhoRd+dhj+R+q4CVETGh8/hmZj1t+PDhdHR0sHDhwt7OSpcNHjyY4cOHd2mcMpXUX5V0N/DW3OuLEXFdiWlPI71S9Ps1hv8N2D8inpJ0CDAF2Lcw/ICIWFRiPmZmPWLgwIGMGTOmt7PRY0q1yhoRvwF+05UJR8TNkkbXGV68V+s2oGuhzczMWqpMHURP+CDw68LvAK6XNEvSSfVGlHRSpZ2ovnjZZ2bWrnr9vQ6SDiAFiDcWer8xIhZI2g64QdIDEXFztfEjYgqpeIoJEyb0rZojM7M21qtXEJLGARcBh0fE4kr/iFiQP58Argb26Z0cmpmtv8q8Ue4eUpHPWoOAiIhx3ZmxpJHAz4APRMRfCv03BTaIiGfz94OAs7szDzMz674yRUyVuoHL8uex+fO79UaSdAUwERgiqQM4ExgIEBEXAGcA2wLfye9irdzOuj1wde63IfDDXEluZmY9qEyAODAiXlP4/VlJd0XEZ+uNFBHHNBh+InBilf4PA68ukS8zM2uhMnUQyk18V378a8nxzMysDytzBfFBYGqhue+ngf9oWY7MzKwtlHmSehapue8t8+9nWp4rMzPrdWWa+95e0sXAjyLiGUm7S/pgD+TNzMx6UZm6hGnAdcCO+fdfgP9qUX7MzKxNlAkQQyLix8BLABGxktTKqpmZ9WNlAsTzkrYlPywn6XWA6yHMzPq5MncxfRyYDrxC0q3AUODIlubKzMx6Xd0AIWkAsH/uXklqXuPBiFjRA3kzM7NeVLeIKSJWAcdExMqImBsR9zo4mJmtH8oUMd0qaTJwJfB8pWdE3NWyXJmZWa8rEyDG589ii6oBvLnpuTEzs7ZRM0BIOj0ivgX8v4j4Qw/myczM2kC9OogT8uf5PZERMzNrL/WKmO6X9BCwo6Q5hf7r9KIgMzPrG2oGiIg4RtIOpGY2Duu5LJmZWTuoW0kdEY/jl/eYma2XWvriH0lTJT0h6d4awyXpfEnzJM2RtHdh2HGSHsrdca3Mp5mZra3Vb4abBhxcZ/ghwC65O4n8nmtJ25DeYb0vsA9wpqStW5pTMzNbQ0sDRETcDDxZJ8nhwPcjuQ3YStIw4G3ADRHxZEQ8BdxA/UBjZmZNVu85iF+QW3CtJiKaUXG9EzC/8Lsj96vV38zMeki9Sur/zZ/vAnYALs+/jwH+2cpMdYWkk0jFU4wcObKXc2Nm1n/Uu8319wCSvhYREwqDfiFpZpPmvwAYUfg9PPdbAEzs1H9GjXxOAaYATJgwoeYVj5mZdU2ZOohNJe1c+SFpDLBpk+Y/Hfj3fDfT64BnIuIx0rMXB0naOldOH5T7mZlZDynTWN/HgBmSHiY9RT0K+HCZiUu6gnQlMERSB+nOpIEAEXEBcC3wdmAesJTcvEdEPCnpi8CdeVJnR0S9ym4zM2uyhgEiIn4jaRfgVbnXAxHxYpmJR8QxDYYH8J81hk0FppaZj5mZNV/DIiZJmwCfAk6JiLuBkZIObXnOzMysV5Wpg7gEWA68Pv9eAHypZTkyM7O2UCZAvCIizgVWAETEUlJdhJmZ9WNlAsRySRuTH5qT9AqgVB2EmZn1XWXuYjoL+A0wQtIPgDcAx7cwT2Zm1gbK3MV0vaRZwOtIRUunR8SilufMzMx6VZm7mH4H7BsRv4qIX0bEIklTeiBvZmbWi8rUQYwBPiPpzEK/CbUSm5lZ/1AmQDwNvAXYXtIvJG3Z2iyZmVk7KBMgFBErI+Jk4CrgD8B2rc2WmZn1tjJ3MV1Q+RIR0yTdQ43mMczMrP+o98KgLSJiCfCT/ArQir8Bn2x5zszMrFfVu4L4IXAoMIv0kFzx6ekAdq42kpmZ9Q/1Xhh0aP4c03PZMTOzdlGviGnveiNGxF3Nz46ZmbWLekVMX6szLIA3NzkvZmbWRuoVMR3QkxkxM7P2UuY2VyTtCewODK70i4jvtypTZmbW+xoGiNzExkRSgLgWOIT0sFzDACHpYOBbwADgooj4Sqfh3wAqVyqbANtFxFZ52CrgnjzsHxFxWOPFMTOzZilzBXEk8GrgzxFxgqTtgcsbjSRpAPBt4ECgA7hT0vSIuK+SJiI+Vkh/KvCawiSWRcT4UkthZmZNV6apjWUR8RKwUtIWwBPAiBLj7QPMi4iHI2I58CPg8DrpjwGuKDFdMzPrAWUCxExJWwHfIz00dxfwpxLj7QTML/zuyP3WImkUqdXYGwu9B0uaKek2Se+sNRNJJ+V0MxcuXFgiW2ZmVkaZFwadnL9eIOk3wBYRMafJ+Tga+GlErCr0GxURCyTtDNwo6Z6I+GuV/E0BpgBMmDAhmpwvM7P1Vtm7mMYBoyvpJY2NiJ81GG0BaxZFDc/9qjmaTg0ARsSC/PmwpBmk+om1AoSZmbVGmbuYpgLjgLnAS7l3AI0CxJ3ALpLGkALD0cD7qkz/VcDWFIqtJG0NLI2IFyUNIb0H+9yGS2NmZk1T5gridRGxe1cnHBErJZ0CXEe6zXVqRMyVdDYwMyKm56RHAz+KiGLx0G7AhZJeItWTfKV495OZmbVemQDxJ0m7d+cEHRHXkp6dKPY7o9Pvs6qM90dgr67Oz8zMmqdMgPg+KUg8DrxIavY7ImJcS3NmZma9qkyAuBj4AOmp5pcapDUzs36iTIBYWKgvMDOz9USZAPFnST8EfkEqYgKgxG2uZmbWh5UJEBuTAsNBhX5lbnM1M7M+rG6AyA3uLY6IT/ZQfszMrE3UbYspN33xhh7Ki5mZtZEyRUyzJU0HfgI8X+npOggzs/6tTIAYDCxmzXdQuw7CzKyfK9Oa6wk9kREzM2svDd8HIWm4pKslPZG7qyQN74nMmZlZ7ynzwqBLgOnAjrn7Re5nfZWUOjOzOsoEiKERcUlErMzdNGBoi/NlZma9rEyAWCzp/ZIG5O79pEprMzPrx8oEiP8A3gs8DjwGHAm44trMrJ8rcxfT34HDeiAvth6QxJrvhjKzdlUzQEg6o9Yw0vsgvtiC/JiZWZuoV8T0fJUO4IPAZ8pMXNLBkh6UNE/SZ6sMP17SQkmzc3diYdhxkh7K3XGll8jMzJqi5hVERHyt8l3S5sDppLqHHwFfqzVeYZwBwLeBA4EO4E5J06u8uvTKiDil07jbAGcCE0hPbc/K4z5Vaqms7fiuWrO+p24ltaRtJH0JmEMKJntHxGci4okS094HmBcRD0fEclJgObxkvt4G3BART+agcANwcMlxzcysCWoGCEnnAXcCzwJ7RcRZXfwHvxMwv/C7I/fr7N2S5kj6qaQRXRwXSSdJmilp5sKFC7uQPTMzq6feFcQnSE9O/zfwqKQluXtW0pImzf8XwOiIGEe6Sri0qxOIiCkRMSEiJgwd6uf3ukKTXO5j1hQSmqR+d0zVDBARsUFEbBwRm0fEFoVu84jYosS0FwAjCr+H537FeSyOiMprTC8CXlt2XDMza60yD8p1153ALpLGSNoIOJrUptPLJA0r/DwMuD9/vw44SNLWkrYmve70uhbm1bpJEnINtFm/VOZ9EN0SESslnUI6sQ8ApkbEXElnAzMjYjpwmqTDgJXAk8DxedwnJX2RFGQAzo6IJ1uVVzMzW1vLAgRARFwLXNup3xmF758DPldj3KnA1Fbmz8ys2WbMSFfUEyf2/RYDWlnEZGZmfVhLryDMKv+m0vOOZtaX+ArCzMyqcoDoR/rjfdhm1nscIMzMrCoHCGs+Pxdh1i+4ktq6xTHArL7KMdKX34/lK4j+QGq7M7brQ8z6PgcIM7PO2vBPV29wgLC24CuO1nKbWdYdDhC9yH9S6PpK8EprSw5A/ZMrqXtBq58unjFD/aIdGFt3PmfbuvAVhPVJLpKynrC+72MOEO2oB4tRZsxQ4YrGzGw1Bwjr02bMkKslepHXff/mAGFtxVc01t/05Qp8BwjrN/rygWjWjloaICQdLOlBSfMkfbbK8I9Luk/SHEm/kzSqMGyVpNm5m955XKvPl/5mzbE+X9W27DZXSQOAbwMHAh3AnZKmR8R9hWR/BiZExFJJHwXOBY7Kw5ZFxPhW5a83VO6IuGn/Xs6IWT8l9e22j9pNK68g9gHmRcTDEbEc+BFweDFBRNwUEUvzz9uA4S3MT+/x33mzHuOixuZpZYDYCZhf+N2R+9XyQeDXhd+DJc2UdJukd9YaSdJJOd3MhQsXrlOGzeoFc5941m+VXWN92g/a4klqSe8HJgDFwpdREbFA0s7AjZLuiYi/dh43IqYAUwAmTJjgi0uzLJWbrx+HRL9+93kvthveygCxABhR+D0891uDpLcCXwD2j4gXK/0jYkH+fFjSDOA1wFoBoj+r1FnEme2z0/frA9HM1tDKIqY7gV0kjZG0EXA0sMbdSJJeA1wIHBYRTxT6by1pUP4+BHgDUKzcNuv/XHfV0PrU5EpvLGvLriAiYqWkU4DrgAHA1IiYK+lsYGZETAfOAzYDfpLL9P4REYcBuwEXSnqJFMS+0unuJ7N1VilHDt/2YlZVS+sgIuJa4NpO/c4ofH9rjfH+COzVyry1E0m9fpLqD69HtPVIZYc9q1dz0e+1RSW1WTtzKY+tr9zUhlkVfblcuyfKqtfnp4vXJ76CsFLWx6fA18c7tlzU2P4q+2VPvBTMVxDWllpZrOObgxpbnx4Gs9ocIPoYH7hmBj3zJ8cBwtbgAGRmFQ4QZr3MRV7WXa3+Q+dK6j6gUil1wAFdr5Tyw2BWRksq5CV0Vp5qGzUXY+X5CsLMzKpygLD1Sl+8d399am/I2osDhDXmAvKWaPrDZl2tzOjByg8/WNc3OUCYWe9zTX1bciW12XpOk7RePSHf63IgrFTgV9Z9TzwZ3VW+grD1Ql8ox+/K7Yq9+bxKu69Hax4HCOvf2rDooi8Eq96yPq+bNtxVHSCsffW7p7rb8QxgVocDhFkf02sxpgkBzjGysXb6U+QAYWY9rp1OglZbSwOEpIMlPShpnqTPVhk+SNKVefjtkkYXhn0u939Q0ttamU+zbiuc6Fp9r//69CxBcVlbVdRYuZrpd0WZTdSy21wlDQC+DRwIdAB3SpoeEfcVkn0QeCoixko6GvgqcJSk3YGjgT2AHYHfSto1Ila1Kr9m3dX5ZUqrzzW9/65xW9Oa28rbppFWXkHsA8yLiIcjYjnwI+DwTmkOBy7N338KvEUplB8O/CgiXoyIvwHz8vTMbD3iP/a9S636hyPpSODgiDgx//4AsG9EnFJIc29O05F//xXYFzgLuC0iLs/9LwZ+HRE/rTKfk4CT8s9XAg+uY9aHAIucvu3z0ur07ZSXdkvfTnlpdfp2ykt30pcxKiKGVhvQ55+kjogpwJRmTU/SzIiY4PTtnZdWp2+nvLRb+nbKS6vTt1NeupN+XbWyiGkBMKLwe3juVzWNpA2BLYHFJcc1M7MWamWAuBPYRdIYSRuRKp2nd0ozHTgufz8SuDFSmdd04Oh8l9MYYBfgjhbm1czMOmlZEVNErJR0CnAdMACYGhFzJZ0NzIyI6cDFwGWS5gFPkoIIOd2PgfuAlcB/9uAdTF0trlqf0rdTXlqdvp3y0m7p2ykvrU7fTnnpTvp10rJKajMz69v8JLWZmVXlAGFmZtVFxHrbAauA2cC9wE+ATXL/qcATwL2N0pPutrqJVF8yFzi9QfrBpAr3u3P6SY3yk4cNAP4M/LJE/h8B7snDZjZIuxXpIcUHgPuB19fJ+ytzv0q3BPivBtP/WF7Oe4ErgMEN0p+e+80FXiq7ffL07gGeA54Ffgds3WBbLiM9TvvbetPO/V8CXsjdAmBYifTLcvc4sHPJfeuxnKcRDZb1cWAF8DRpX3p7g2l3AC8CzwBf70LenwfuLpH353P6J4H9SqZ/AXgU2J4ax1GN7bpntbS1tmutadfariXSr7FdG6Rfa7uWmP7yPP2ngSPqpc/jnEo6fucC5zb1HNmbJ+je7oDnCt9/AHw8f38TsHeVHXut9HmH2jv32xz4C7B7nfQCNsv9BgK3A6+rl5/8++PAD1kzQNTK/yPAkJLLeilwYv6+EbBVo7zkfgPyATKqzrLuBPwN2Dj3/zFwfJ30e5KCwSakGyhWAmPLbB/SCeRc4LM57S9JTbfU2pbPkwLeDODXJbb9MmDD/H0u8Lsu7CszgT82Sk86EVyXv/+/Bst6FvDJRuulkPffAoNy+jO6kPf7SQ+p1ku/Ejgkf7+J1IJCvfSrgP3z9z8B11PjOKqxXSdXS1tru9aadq3t2iD9Wtu1UfrO27VB+uXAJ7twjjmgsm3z7+2aeY50EdNqtwBjASLiZtI/oYbpI+KxiLgrj/cs6YDaqU76iIjncr+Buat2p8DL+ZE0HHgHcFGZ/JdwCzBW0pakg/jinP/lEfF0yWm/BfhrRPy9QfoNgY3zcy6bkP4x1kq/G3B7RCyNiJWkE8m7Ok+zzvY5nBTwbiE9bfrOOmkjIipP3f+5xLRX5TxV8jKiQfqi+cAWJdJ/A/g06Z/+mJLTL7NeNgS+EhEv5vQ7lJl2bvZm+0qaBum3yJ9/Jf0Lrpd+A+Dm/H06MKHBcdR5ux5YJ+1a27XBtNfarl04pucDW5RIv8Z27cY5o176j7J62xIRT1SZTrc5QPDyQ3qHkC5lu50+t0b7GtJVQc30kgZImk26/L4hIuqmB75J2sFeKpmfAK6XNCs3RVIr7RhgIXCJpD9LukjSpmWWlXRL8hX18hIRC4D/Bf5BusR+JiKurzP9e4H9JG0rqXIVMaIL22f7vDyHkLbB9g3SQ7qie0OJaRfz+07g1hJpz5E0nxRML22QfACpiGMusDGpOKGRU0hXSeMlbV0n3Qak9Xo78CVS0WAZE0nr5w8N0r0InJeX9VgaL+tLwOF5XR5L+uPwsirHUc3tWuuYo8Z2bXCMvpNO27Va+nrbtUr6utu1Rn5OkTQH+Bwp4NZLvyt520r6vaR/oZmaeTnS1zpWlw/OBv4P2KgwbDS1y4mrpd8MmAW8q0z6PHwr0iX5nrXSA4cC38nDJ1K9DmKN6QM75c/tSOXTb6ox7Qmk4oF9c/pvAV8ssawbkf7JbV8vL6Q6gBuBoaQrpWuA9zfI+wfzeryZdCJZWGb75OmtYs35P1ViWz4HXFlv2p3SP0Y6aLuyr/yxsl5r5H0TUlCfw+q6nWENlvXewrJ+mfScUa28RGE9Xkkq9lOJvC8i/YtttKwv5WnOJhXr/K7EulkCLCWd6BbXOo4abNd6x1y17VovfbXt2uiY7rxdO+e90XZtNP07gWkN0t+b14tIDZq+vG2bco5s1oT6YkehPLHKsGo7dtX0pJPfdaxdTl9z+oU0Z7C6zHGt9KSDv4NUr/B4Pqgu78L0zyKVVVeb9g7AI4Xf+wG/KrFuDgeub7SswHuAiwu//53Vwa5M3pcDJ5fZPvmE8CCrK4+HAQ822paksuoJZbY9cDypzHyTMukL30d2ymvnvO9FOsk+kruVpKuuHWota63518jLSuCAwu+/AkMbLOuGwD+B4SWWNVgdcAQs6cK62RW4o9ZxVGu7Vktbb7vWS19tuzaafuftWiPvNbdryemPrjf93P83tbZtMzoXMa2jXE57MXB/RHy9RPqhkrbK3zcmvS/jgVrpI+JzETE8IkaTinVujIj315n+ppI2r3wHDiL9y6g27ceB+ZJemXu9hXJFG8dQpXipin8Ar5O0SV5PbyGVn9Ykabv8OZJ0kvphiflUFJtuOQ74eRfGbWQAqZjvsIhYWiJ9saHqw6m/je8BlkbE6LydO0iVko/XnLg0rPDzCGps42wlqTITSbuy+gqwnrcCD0RuabmBAPbP398MPNQgvXJeNgD+G7igwXFUbbt25ZirN+21tmuD9Gtt11rpa21XUuAtM/0jgHsb5Ocaur5ty2tWpOmLHbWvCK4gXXKuyBv1g7XSA29kzcvI2cDb66QfR6o8m0M6qM9olJ/C8InUuIup0G9nUrFS5TbaLzRY1vGkuzHm5J1t6wbpNyU1qLhlyXU5iXRyvBe4jNV3W9RKfwspSN0NLCu7fUj/BLcl3YXyEOnOjm3qbMtlrL7185/AdQ22/UukSsnKNr6gQfqVeZnnAL9gdbFfw32Lwl1odZb1MlL5+hzSCXRYvWkDl+f83AW8uVFegGnAR0oeF0tJRR93k4qMXtsg/YukO3H+AnyFdFKsehzV2K6HVEtba7vWmnat7dog/VrbtUH6tbZrg/QrWHu71ku/EVW2bbM6N7VhZmZVuYjJzMyqcoAwM7OqHCDMzKwqBwgzM6vKAcLMzKpygDDrJkmrJM2WdK+kn+TnPSZIOj8PnyjpXxtMY7Skes8wmPUaBwiz7lsWEeMjYk/SU98fiYiZEXFaHj4RqBsgzNqZA4RZc1RayJ0o6Ze5UbWPAB/LVxn7Sdpe0tWS7s5dJXgMkPQ9SXMlXZ+fsDfrdQ4QZuuoWmuzEfEI6ancb+SrjFuA84HfR8SrSU0uzM3JdwG+HRF7kF4S8+6ey71ZbQ4QZt23cW62fSap3amLG6R/M/BdgIhYFRHP5P5/i4jZ+fssUiNtZr1uw97OgFkftiwixhd7pHbVuuzFwvdVpPcGmPU6X0GYtc6zpFdEVvyO9AawykujtuyVXJmV5ABh1jq/AI6oVFIDpwMHSLqHVJS0e6/mzqwBt+ZqZmZV+QrCzMyqcoAwM7OqHCDMzKwqBwgzM6vKAcLMzKpygDAzs6ocIMzMrKr/D+EyRRR1CZMLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels=[]\n",
    "for i in np.arange(26):\n",
    "    labels.append(\"P\"+str(i+1))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.15\n",
    "colors = ['r', 'g', 'y', 'b', 'black']\n",
    "plots = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    Xs = np.asarray(np.abs(audio_mfcc[0][i])).reshape(-1)\n",
    "    p = ax.bar(ind + i*width, Xs, width, color=colors[i])\n",
    "    plots.append(p[0])\n",
    "\n",
    "xticks = ind + width / (audio_mfcc.shape[0])\n",
    "print(xticks)\n",
    "ax.legend(tuple(plots), ('S1', 'S2', 'S3', 'S4', 'S5'))\n",
    "ax.yaxis.set_units(inch)\n",
    "ax.autoscale_view()\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_ylabel('Normalized freq coumt')\n",
    "ax.set_xlabel('Pitch')\n",
    "ax.set_title('Normalized frequency counts for Various Sounds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 4.30906334e+01 -2.03534174e+00  2.13124916e-01 ... -1.80211738e-01\n",
      "   -1.85429707e-01 -2.71477193e-01]\n",
      "  [ 4.31706581e+01 -2.00894237e+00  1.24617569e-01 ... -1.76925331e-01\n",
      "   -1.90575659e-01 -2.57524341e-01]\n",
      "  [ 4.31397552e+01 -1.99346173e+00  2.48030107e-02 ... -1.74366608e-01\n",
      "   -2.01282144e-01 -2.28919283e-01]\n",
      "  ...\n",
      "  [ 3.75711555e+01  1.11948147e-01 -1.20602953e+00 ... -1.09063618e-01\n",
      "   -1.47348404e-01 -4.01394591e-02]\n",
      "  [ 3.72089195e+01  1.13353238e-01 -1.15313697e+00 ... -1.19969286e-01\n",
      "   -1.65252820e-01 -3.10349297e-02]\n",
      "  [ 3.66761971e+01  7.20925033e-02 -1.06190836e+00 ... -1.32965922e-01\n",
      "   -1.79992720e-01 -2.88166851e-02]]]\n",
      "(1, 165, 26)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c818939da44aceb658580bead2f025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=165.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train inputs shape (165, 494) \n",
      "Train inputs [[-0.1549316  -0.1549316  -0.1549316  ... -0.16783054 -0.17190124\n",
      "  -0.15780927]\n",
      " [-0.1549316  -0.1549316  -0.1549316  ... -0.16929814 -0.1677892\n",
      "  -0.16038078]\n",
      " [-0.1549316  -0.1549316  -0.1549316  ... -0.17141952 -0.16438814\n",
      "  -0.16207673]\n",
      " ...\n",
      " [ 4.868814   -0.3211337  -0.11208492 ... -0.1549316  -0.1549316\n",
      "  -0.1549316 ]\n",
      " [ 4.8369946  -0.31495303 -0.11409124 ... -0.1549316  -0.1549316\n",
      "  -0.1549316 ]\n",
      " [ 4.819872   -0.30001065 -0.13118725 ... -0.1549316  -0.1549316\n",
      "  -0.1549316 ]]\n"
     ]
    }
   ],
   "source": [
    "filename =  '../data/LibriSpeech/train-clean-100/3486/166424/3486-166424-0004.wav'\n",
    "raw_audio = tf.io.read_file(filename)\n",
    "audio, fs = decode_wav(raw_audio)\n",
    "\n",
    "wsize = 16384 #1024\n",
    "stride = 448 #64\n",
    "\n",
    "\n",
    "# Get mfcc coefficients\n",
    "spectrogram = AudioSpectrogram(\n",
    "        input=audio, window_size=wsize,stride=stride)\n",
    "numcep=26\n",
    "numcontext=9\n",
    "orig_inputs = Mfcc(spectrogram=spectrogram, sample_rate=fs, dct_coefficient_count=numcep) \n",
    "orig_inputs = orig_inputs[:,::2]\n",
    "\n",
    "audio_mfcc = orig_inputs.numpy()\n",
    "print(audio_mfcc)\n",
    "print(np.shape(audio_mfcc))\n",
    "\n",
    "train_inputs = np.array([], np.float32)\n",
    "train_inputs.resize((audio_mfcc.shape[1], numcep + 2 * numcep * numcontext))\n",
    "\n",
    "# Prepare pre-fix post fix context\n",
    "empty_mfcc = np.array([])\n",
    "empty_mfcc.resize((numcep))\n",
    "empty_mfcc = tf.convert_to_tensor(empty_mfcc, dtype=tf.float32)\n",
    "empty_mfcc_ev = empty_mfcc.numpy()\n",
    "\n",
    "# Prepare train_inputs with past and future contexts\n",
    "# This code always takes 9 time steps previous and 9 time steps in the future along with the current time step\n",
    "time_slices = range(train_inputs.shape[0])\n",
    "context_past_min = time_slices[0] + numcontext #starting min point for past content, has to be at least 9 ts\n",
    "context_future_max = time_slices[-1] - numcontext  #ending point  max for future content, size time slices - 9ts\n",
    "\n",
    "for time_slice in tqdm(time_slices):\n",
    "    #print('time slice %d ' % (time_slice))\n",
    "    # Reminder: array[start:stop:step]\n",
    "    # slices from indice |start| up to |stop| (not included), every |step|\n",
    "\n",
    "    # Add empty context data of the correct size to the start and end\n",
    "    # of the MFCC feature matrix\n",
    "\n",
    "    # Pick up to numcontext time slices in the past, and complete with empty\n",
    "    # mfcc features\n",
    "    need_empty_past = max(0, (context_past_min - time_slice))\n",
    "    empty_source_past = np.asarray([empty_mfcc_ev for empty_slots in range(need_empty_past)])\n",
    "    data_source_past = orig_inputs[0][max(0, time_slice - numcontext):time_slice]\n",
    "    assert(len(empty_source_past) + data_source_past.numpy().shape[0] == numcontext)\n",
    "\n",
    "    # Pick up to numcontext time slices in the future, and complete with empty\n",
    "    # mfcc features\n",
    "    need_empty_future = max(0, (time_slice - context_future_max))\n",
    "    empty_source_future = np.asarray([empty_mfcc_ev for empty_slots in range(need_empty_future)])\n",
    "    data_source_future = orig_inputs[0][time_slice + 1:time_slice + numcontext + 1]\n",
    "    assert(len(empty_source_future) + data_source_future.numpy().shape[0] == numcontext)\n",
    "\n",
    "    # pad if needed for the past or future, or else simply take past and future\n",
    "    if need_empty_past:\n",
    "        past = tf.concat([tf.cast(empty_source_past, tf.float32), tf.cast(data_source_past, tf.float32)], 0)\n",
    "    else:\n",
    "        past = data_source_past\n",
    "\n",
    "    if need_empty_future:\n",
    "        future = tf.concat([tf.cast(data_source_future, tf.float32), tf.cast(empty_source_future, tf.float32)], 0)\n",
    "    else:\n",
    "        future = data_source_future\n",
    "\n",
    "\n",
    "    past = tf.reshape(past, [numcontext*numcep])\n",
    "    now = orig_inputs[0][time_slice]\n",
    "    future  = tf.reshape(future, [numcontext*numcep])\n",
    "\n",
    "    train_inputs[time_slice] = np.concatenate((past.numpy(), now.numpy(), future.numpy()))\n",
    "    assert(train_inputs[time_slice].shape[0] == numcep + 2*numcep*numcontext)\n",
    "\n",
    "train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n",
    "print('Train inputs shape %s ' % str(np.shape(train_inputs)))\n",
    "print('Train inputs '+str(train_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = {\n",
    "    'data_dir': train_path,\n",
    "    'cache_dir' : '../data/cache/LibriSpeech',\n",
    "    'window_size': 20,\n",
    "    'step_size': 10\n",
    "}\n",
    "\n",
    "model = {\n",
    "    'verbose': 1,\n",
    "    'conv_channels': [100],\n",
    "    'conv_filters': [5],\n",
    "    'conv_strides': [2],\n",
    "    'rnn_units': [64],\n",
    "    'bidirectional_rnn': True,\n",
    "    'future_context': 2,\n",
    "    'use_bn': True,\n",
    "    'learning_rate': 0.001\n",
    "\n",
    "}\n",
    "\n",
    "training = {\n",
    "    'tensorboard': False,\n",
    "    'log_dir': './logs',\n",
    "    'batch_size': 5,\n",
    "    'epochs': 5,\n",
    "    'validation_size': 0.2,\n",
    "    'max_train' : 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(preprocessing['cache_dir']):\n",
    "    os.makedirs(preprocessing['cache_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_relu(x):\n",
    "    return tf.keras.activations.relu(x, max_value=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc(y_true, y_pred):\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechModel(object):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "\n",
    "        input_data = tf.keras.layers.Input(name='inputs', shape=[hparams['max_input_length'], 161])\n",
    "        x = input_data\n",
    "\n",
    "        if hparams['use_bn']:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = tf.keras.layers.ZeroPadding1D(padding=(0, hparams['max_input_length']))(x)\n",
    "        for i in range(len(hparams['conv_channels'])):\n",
    "            x = tf.keras.layers.Conv1D(hparams['conv_channels'][i], hparams['conv_filters'][i],\n",
    "                                       strides=hparams['conv_strides'][i], activation='relu', padding='same')(x)\n",
    "\n",
    "        if hparams['use_bn']:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        for h_units in hparams['rnn_units']:\n",
    "            if hparams['bidirectional_rnn']:\n",
    "                h_units = int(h_units / 2)\n",
    "            gru = tf.keras.layers.GRU(h_units, activation='relu', return_sequences=True)\n",
    "            if hparams['bidirectional_rnn']:\n",
    "                gru = tf.keras.layers.Bidirectional(gru, merge_mode='sum')\n",
    "            x = gru(x)\n",
    "\n",
    "        if hparams['use_bn']:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        if hparams['future_context'] > 0:\n",
    "            if hparams['future_context'] > 1:\n",
    "                x = tf.keras.layers.ZeroPadding1D(padding=(0, hparams['future_context'] - 1))(x)\n",
    "            x = tf.keras.layers.Conv1D(100, hparams['future_context'], activation='relu')(x)\n",
    "\n",
    "        y_pred = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(hparams['vocab_size'] + 1,\n",
    "                                                                       activation='sigmoid'))(x)\n",
    "\n",
    "        labels = tf.keras.layers.Input(name='labels', shape=[None], dtype='int32')\n",
    "        input_length = tf.keras.layers.Input(name='input_lengths', shape=[1], dtype='int32')\n",
    "        label_length = tf.keras.layers.Input(name='label_lengths', shape=[1], dtype='int32')\n",
    "\n",
    "        loss_out = tf.keras.layers.Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred,\n",
    "                                                                                           labels,\n",
    "                                                                                           input_length,\n",
    "                                                                                           label_length])\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[input_data, labels, input_length, label_length], outputs=[loss_out])\n",
    "\n",
    "        if hparams['verbose']:\n",
    "            print(self.model.summary())\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=hparams['learning_rate'], beta_1=0.9, beta_2=0.999,\n",
    "                                             epsilon=1e-8, clipnorm=5)\n",
    "\n",
    "        self.model.compile(optimizer=optimizer, loss=ctc)\n",
    "\n",
    "    def train_generator(self, generator, train_params):\n",
    "        callbacks = []\n",
    "\n",
    "        if train_params['tensorboard']:\n",
    "            callbacks.append(tf.keras.callbacks.TensorBoard(train_params['log_dir'], write_images=True))\n",
    "\n",
    "        self.model.fit(generator, epochs=train_params['epochs'],\n",
    "                                 steps_per_epoch=train_params['steps_per_epoch'],\n",
    "                                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_mapping():\n",
    "    character_map = {' ': 0}\n",
    "\n",
    "    for i in range(97, 123):\n",
    "        character_map[chr(i)] = len(character_map)\n",
    "\n",
    "    return character_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_details(filename):\n",
    "    result = {\n",
    "        'max_input_length': 0,\n",
    "        'max_label_length': 0,\n",
    "        'num_samples': 0\n",
    "    }\n",
    "\n",
    "    # Get max lengths\n",
    "    with open(filename, 'r') as metadata:\n",
    "        metadata_reader = csv.DictReader(metadata, fieldnames=['filename', 'spec_length', 'labels_length', 'labels'])\n",
    "        next(metadata_reader)\n",
    "        for row in metadata_reader:\n",
    "            if int(row['spec_length']) > result['max_input_length']:\n",
    "                result['max_input_length'] = int(row['spec_length'])\n",
    "            if int(row['labels_length']) > result['max_label_length']:\n",
    "                result['max_label_length'] = int(row['labels_length'])\n",
    "            result['num_samples'] += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(directory, max_input_length, max_label_length, batch_size=64, num_epochs=5):\n",
    "    x, y, input_lengths, label_lengths = [], [], [], []\n",
    "    epochs = 0\n",
    "    \n",
    "    while epochs < num_epochs:\n",
    "        with open(os.path.join(directory, 'LibriSpeech-metadata.csv'), 'r') as metadata:\n",
    "            metadata_reader = csv.DictReader(metadata, fieldnames=['filename', 'spec_length', 'labels_length', 'labels'])\n",
    "            next(metadata_reader)\n",
    "            for row in metadata_reader:\n",
    "                audio = np.load(os.path.join(directory, row['filename'] + '.npy'))\n",
    "                x.append(audio)\n",
    "                y.append([int(i) for i in row['labels'].split(' ')])\n",
    "                input_lengths.append(row['spec_length'])\n",
    "                label_lengths.append(row['labels_length'])\n",
    "                if len(x) == batch_size:\n",
    "                    yield {\n",
    "                        'inputs': tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=max_input_length, padding='post'),\n",
    "                        'labels': tf.keras.preprocessing.sequence.pad_sequences(y, maxlen=max_label_length, padding='post'),\n",
    "                        'input_lengths': np.asarray(input_lengths, dtype=np.int32),\n",
    "                        'label_lengths': np.asarray(label_lengths, dtype=np.int32)\n",
    "                    }, {\n",
    "                        'ctc': np.zeros([batch_size])\n",
    "                    }\n",
    "                    x, y, input_lengths, label_lengths = [], [], [], []\n",
    "        epochs = epochs + 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_linear_specgram(audio, sample_rate, window_size=20,\n",
    "                        step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "\n",
    "    _, _, spec = signal.spectrogram(audio, fs=sample_rate,\n",
    "                                    window='hann', nperseg=nperseg, noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "\n",
    "    return np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "\n",
    "def preprocess_librispeech(directory):\n",
    "    print(\"Pre-processing LibriSpeech corpus\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    character_mapping = create_character_mapping()\n",
    "\n",
    "    if not os.path.exists(preprocessing['data_dir']):\n",
    "        os.makedirs(preprocessing['data_dir'])\n",
    "\n",
    "    dir_walk = list(os.walk(directory))\n",
    "    num_hours = 0\n",
    "    num_train = 0\n",
    "\n",
    "    with open(os.path.join(preprocessing['cache_dir'] + '/LibriSpeech-metadata.csv'), 'w', newline='') as metadata:\n",
    "        metadata_writer = csv.DictWriter(metadata, fieldnames=['filename', 'spec_length', 'labels_length', 'labels'])\n",
    "        metadata_writer.writeheader()\n",
    "        for root, dirs, files in tqdm(dir_walk):\n",
    "            for file in files:\n",
    "                if file[-4:] == '.txt' and num_train < training['max_train']:\n",
    "                    filename = os.path.join(root, file)\n",
    "                    with open(filename, 'r') as f:\n",
    "                        txt = f.read().split(' ')\n",
    "                        \n",
    "                        filename_base_no_path = os.path.splitext(file)[0]\n",
    "                        filename_base = os.path.splitext(filename)[0]\n",
    "                        filename_wav = filename_base + '.wav'\n",
    "                        audio, sr = sf.read(filename_wav)\n",
    "                        num_hours += (len(audio) / sr) / 3600\n",
    "                        spec = log_linear_specgram(audio, sr, window_size=preprocessing['window_size'],\n",
    "                                                   step_size=preprocessing['step_size'])\n",
    "                        np.save(os.path.join(preprocessing['cache_dir'], filename_base_no_path) + '.npy', spec)\n",
    "                        ids = [character_mapping[c] for c in ' '.join(txt).lower()\n",
    "                               if c in character_mapping]\n",
    "                        metadata_writer.writerow({\n",
    "                            'filename': filename_base_no_path,\n",
    "                            'spec_length': spec.shape[0],\n",
    "                            'labels_length': len(ids),\n",
    "                            'labels': ' '.join([str(i) for i in ids])\n",
    "                        })\n",
    "                            \n",
    "                        if num_train + 1 <= training['max_train']:\n",
    "                            num_train = num_train + 1\n",
    "                            \n",
    "            \n",
    "            if num_train >= training['max_train']:\n",
    "                print('Processed {} files: max train {} reached...'.format(num_train, training['max_train']))\n",
    "                break\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(\"Hours pre-processed: \" + str(num_hours))\n",
    "    print(\"Time: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing LibriSpeech corpus\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7c41704f4c4154b929c5521c74dcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=837.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 files: max train 100 reached...\n",
      "\n",
      "Done!\n",
      "Hours pre-processed: 0.388797204861111\n",
      "Time: 2.3015971183776855\n"
     ]
    }
   ],
   "source": [
    "preprocess_librispeech(preprocessing['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_mapping = create_character_mapping()\n",
    "data_details = get_data_details(filename=os.path.join(preprocessing['cache_dir'], 'LibriSpeech-metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': 1676, 'max_label_length': 244, 'num_samples': 100}\n"
     ]
    }
   ],
   "source": [
    "print(data_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['steps_per_epoch'] = int(data_details['num_samples'] / training['batch_size'])\n",
    "model['max_input_length'] = data_details['max_input_length']\n",
    "model['max_label_length'] = data_details['max_label_length']\n",
    "model['vocab_size'] = len(character_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = create_data_generator(directory=preprocessing['cache_dir'],\n",
    "                                             max_input_length=model['max_input_length'],\n",
    "                                             max_label_length=model['max_label_length'],\n",
    "                                             batch_size=training['batch_size'],\n",
    "                                             num_epochs=training['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, 1676, 161)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1676, 161)    644         inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 3352, 161)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1676, 100)    80600       zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1676, 100)    400         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1676, 32)     25728       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1676, 32)     128         bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPadding1D (None, 1677, 32)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1676, 100)    6500        zero_padding1d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 1676, 28)     2828        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "labels (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_lengths (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_lengths (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           time_distributed[0][0]           \n",
      "                                                                 labels[0][0]                     \n",
      "                                                                 input_lengths[0][0]              \n",
      "                                                                 label_lengths[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 116,828\n",
      "Trainable params: 116,242\n",
      "Non-trainable params: 586\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 729ms/step - loss: 3150.2871\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 15s 727ms/step - loss: 2122.6577\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 757ms/step - loss: 1315.9285\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 942.1353\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 33s 2s/step - loss: 723.4091\n"
     ]
    }
   ],
   "source": [
    "speech_model = SpeechModel(hparams=model)\n",
    "speech_model.train_generator(data_generator, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_gen = create_data_generator(directory=preprocessing['cache_dir'],\n",
    "                                             max_input_length=model['max_input_length'],\n",
    "                                             max_label_length=model['max_label_length'],\n",
    "                                             batch_size=training['batch_size'],\n",
    "                                             num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[743.97205]\n",
      " [759.98737]\n",
      " [956.3779 ]\n",
      " [653.14355]\n",
      " [861.2862 ]]\n",
      "[[952.1141]\n",
      " [815.5294]\n",
      " [921.467 ]\n",
      " [857.4423]\n",
      " [902.2468]]\n",
      "[[ 897.61945]\n",
      " [ 885.2978 ]\n",
      " [1032.0532 ]\n",
      " [ 946.6051 ]\n",
      " [ 712.92255]]\n",
      "[[788.29486]\n",
      " [851.4849 ]\n",
      " [890.17596]\n",
      " [841.6435 ]\n",
      " [698.52325]]\n",
      "[[1038.7188 ]\n",
      " [ 920.17224]\n",
      " [ 881.7219 ]\n",
      " [1093.583  ]\n",
      " [ 944.9488 ]]\n",
      "[[ 933.3971]\n",
      " [1026.4095]\n",
      " [ 963.6623]\n",
      " [ 989.2978]\n",
      " [ 900.0741]]\n",
      "[[1036.6003 ]\n",
      " [ 827.54895]\n",
      " [ 788.2629 ]\n",
      " [ 663.8893 ]\n",
      " [ 975.51385]]\n",
      "[[850.2042]\n",
      " [904.1754]\n",
      " [902.2056]\n",
      " [936.2772]\n",
      " [797.0714]]\n",
      "[[672.27466]\n",
      " [798.2145 ]\n",
      " [485.91864]\n",
      " [896.7823 ]\n",
      " [850.35736]]\n",
      "[[1292.2743 ]\n",
      " [ 957.9069 ]\n",
      " [ 745.57623]\n",
      " [ 691.29156]\n",
      " [ 989.72974]]\n",
      "[[ 731.374 ]\n",
      " [ 852.1066]\n",
      " [1414.2797]\n",
      " [ 861.0073]\n",
      " [ 830.006 ]]\n",
      "[[814.72687]\n",
      " [799.25464]\n",
      " [844.71497]\n",
      " [928.69037]\n",
      " [932.4999 ]]\n",
      "[[399.04126]\n",
      " [614.9378 ]\n",
      " [889.0976 ]\n",
      " [673.6798 ]\n",
      " [812.8345 ]]\n",
      "[[719.9427 ]\n",
      " [745.549  ]\n",
      " [960.40765]\n",
      " [812.5403 ]\n",
      " [772.0764 ]]\n",
      "[[1308.6666 ]\n",
      " [ 918.55383]\n",
      " [ 761.92773]\n",
      " [ 868.2501 ]\n",
      " [ 789.29114]]\n",
      "[[870.848  ]\n",
      " [881.1194 ]\n",
      " [983.18274]\n",
      " [876.6072 ]\n",
      " [908.1066 ]]\n",
      "[[968.3482 ]\n",
      " [775.756  ]\n",
      " [726.08673]\n",
      " [976.6714 ]\n",
      " [823.09015]]\n",
      "[[848.7683 ]\n",
      " [851.99304]\n",
      " [899.56226]\n",
      " [825.3777 ]\n",
      " [843.40515]]\n",
      "[[1104.6473 ]\n",
      " [ 788.1048 ]\n",
      " [ 800.372  ]\n",
      " [ 854.14014]\n",
      " [ 656.94867]]\n",
      "[[681.26404]\n",
      " [889.62085]\n",
      " [675.7146 ]\n",
      " [827.22424]\n",
      " [890.8238 ]]\n"
     ]
    }
   ],
   "source": [
    "for i in tst_gen:\n",
    "    print(speech_model.model.predict(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
